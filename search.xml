<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F10%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8BODS%E5%B1%82%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5-Datax%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[数据仓库之ODS层数据同步-Datax脚本自动生成1.背景​ 最近在搞公司级数据仓库，第一件事就是梳理ODS层表结构。梳理完成后（去掉一些脱敏的字段）就要建ODS层的hive表结构（不同的业务库都有相当多的mysql表—几百张），如果一个个去建表将会是一个很大的工程，当然也会出错，所以我这边就想着如何可以通过mysql的表结构直接生成hive表结构，这样既省时省力，又快速准确。当然还是被我找到了。 ​ 然后将业务库的数据同步到数据仓库的ODS层(hive)。如果一个个去写Datax脚本也将会花费大量的时间，并且非常容易出错，问题也不套排查。因此想着自己能否找到一个脚本，可以实现配置化，直接生成!最后这个也被我找到了。（mysqlToHive代码生成datax的Json脚本） 2.根据mysql表自动生成ods层hive表结构​ 蹩脚的代码（自己python能力加强后再来修改）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#!/usr/bin/env python# -*- coding:utf-8 -*-#@Time : 2019/9/2 16:31#@Author: kobe#@File : mysqlToHive.pyimport pymysqlimport reclass MysqlToHive():def get_table_info(table, schema='ods_xxx', ispartition=True): ''' table = 为表名，mysql,hive表名一致 schema = 为hive中的库名 ispartition : 是否分区默认为分区 ''' cols = [] tables = [] connection = pymysql.connect(host='192.168.x.xx', user='xxxx', password='xxxx', db='xxx', port=3306, charset='utf8' ) try: # 获取一个游标 with connection.cursor(cursor=pymysql.cursors.DictCursor) as cursor: sql = 'SHOW FULL FIELDS FROM &#123;0&#125;'.format(table) sql_name = 'SHOW CREATE TABLE &#123;0&#125;'.format(table) #获取表注释的名称 name_cout = cursor.execute(sql_name) # 返回记录条数 print(name_cout) try: for row in cursor: # cursor.fetchall() # print(row['Create Table'].split("COMMENT=")[1]) if("COMMENT=" in row['Create Table']): table_name = row['Create Table'].split("COMMENT=")[1] else: table_name = '缺少表名称注释' print(table_name) except: print('程序异常!') create_head = '''create table if not exists `&#123;0&#125;_&#123;1&#125;`('''.format(schema, table) if ispartition: create_tail = r'''COMMENT &#123;0&#125;PARTITIONED BY (day INT COMMENT 'yyyyMMdd') ROW FORMAT DELIMITEDFIELDS TERMINATED BY "|"COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'LINES TERMINATED BY '\n'STORED AS TEXTFILE;'''.format(table_name) else: create_tail = r'''COMMENT &#123;0&#125;PARTITIONED BY (day INT COMMENT 'yyyyMMdd') ROW FORMAT DELIMITEDFIELDS TERMINATED BY "|"COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'LINES TERMINATED BY '\n'STORED AS TEXTFILE;'''.format(table_name) cout = cursor.execute(sql) # 返回记录条数 try: for row in cursor: # cursor.fetchall() #print(row['Create Table'].split("COMMENT=")[1]) cols.append(row['Field']) # if 'bigint' in row['Type']: # row['Type'] = "bigint" # elif 'int' in row['Type'] or 'tinyint' in row['Type'] or 'smallint' in row['Type'] or 'mediumint' in \ # row['Type'] or 'integer' in row['Type']: # row['Type'] = "int" # elif 'double' in row['Type'] or 'float' in row['Type'] or 'decimal' in row['Type']: # row['Type'] = "double" # else: # row['Type'] = "string" row['Type'] = "string" create_head += '\n' + '`' + row['Field'] + '`' + ' ' + row['Type'] + ' comment \'' + row['Comment'] + '\' ,' except: print('程序异常!') finally: connection.close() create_str = create_head[:-2] + '\n' + ')' + create_tail + '\n' return cols, create_str # 返回字段列表与你建表语句if __name__ == '__main__': ''' table = 为表名，mysql,hive表名一致 schema = 为hive中的库名 ispartition : 是否分区默认为分区 ''' cols = [] tables = [] connection = pymysql.connect(host='192.168.x.xxx', user='xxx', password='xxx', db='xxx', port=3306, charset='utf8' ) try: # 获取一个游标 with connection.cursor(cursor=pymysql.cursors.DictCursor) as cursor: sql_table = 'SHOW TABLES' # 获取数据库中所有表名称 table_cout = cursor.execute(sql_table) # 返回记录条数 try: for row in cursor: # cursor.fetchall() # print(row['Tables_in_zjs']) tables.append(row['Tables_in_zjs']) except: print('程序异常!') finally: connection.close() try: for i in tables: print(i) cols, create_str = MysqlToHive.get_table_info(i) print(cols) print(create_str) # 写进sql中 with open('ods_xxx_create_table.sql', 'a+',encoding='UTF-8') as f: f.write(create_str) except: print('程序异常!!!') ​ 当你输入了MYSQL的JDBC账号密码及想要得到的库名后就可以生成自己想要的hive建表语句了！ 3.根据mysql表结构和hive表结构自动生产datax脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184package com.xxxx.datawarehouse;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.alibaba.fastjson.serializer.SerializerFeature;import org.apache.commons.lang3.StringUtils;import java.io.BufferedReader;import java.io.BufferedWriter;import java.io.FileReader;import java.io.FileWriter;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Description DataX生成通用Json工具 * @Author: kobe * @Date: 2019/9/5 13:42 */public class DataxJsonUtil &#123; public static void main(String[] args) throws Exception &#123; //读取文件 设定格式:schema|table|target_table|pk_column|columns|add_column //BufferedReader br = new BufferedReader(new FileReader("C:/Users/kobe/Desktop/mysqlReader.txt")); BufferedReader br = new BufferedReader(new FileReader("C:/Users/kobe/Desktop/mysqlReader.txt")); String line = null; //读每行数据 while ((line = br.readLine()) != null) &#123; //文件内容放入数组 String[] info = line.split("\\|"); String target_table = info[2]; //拼装全量Json信息 String str = toJson(line.trim(), "full"); //全量抽数json BufferedWriter bw = new BufferedWriter(new FileWriter("C:/Users/kobe/Desktop/ods/xxx/" + target_table + ".json")); //写入全量文件 bw.write(str); bw.flush(); bw.close(); //拼装增量Json信息 String str1 = toJson(line.trim(), "add"); //增量抽数json BufferedWriter bw1 = new BufferedWriter(new FileWriter("C:/Users/kobe/Desktop/ods/xxx/" + target_table + ".json")); //写入增量文件 bw1.write(str1); bw1.flush(); bw1.close(); &#125; &#125; /** * @description Json信息 * @author kobe * @date 2019/7/31 20:09 */ public static String toJson(String line, String flag) &#123; //文件内容放入数组 String[] info = line.split("\\|"); String schema = info[0]; String tableName = info[1]; String target_table = info[2]; String pkColumn = info[3]; if ("null".equals(pkColumn.toLowerCase())) &#123; pkColumn = null; &#125; String allColumns = info[4]; String addColumn = null;//增量字段 if (info.length &gt; 5) &#123; addColumn = info[5]; &#125; //从最里层往外扩 //********reader部分******** StringBuilder sb = new StringBuilder();// sb.append("jdbc:mysql://127.0.0.1:3306/");// sb.append(schema);// sb.append("?autoReconnect=true");//开启自动重连，防止连接时间短超时 sb.append("$&#123;jdbcUrl&#125;"); Map&lt;String, Object&gt; m1 = new HashMap&lt;&gt;(); List&lt;String&gt; rjdbcList = new ArrayList&lt;&gt;(); rjdbcList.add(sb.toString()); m1.put("jdbcUrl", rjdbcList);//源jdbc信息 List&lt;String&gt; rtableList = new ArrayList&lt;&gt;(); rtableList.add(tableName); m1.put("table", rtableList);//源表名 List&lt;Object&gt; rconnList = new ArrayList&lt;&gt;(); rconnList.add(m1); Map&lt;String, Object&gt; mm1 = new HashMap&lt;&gt;(); mm1.put("column", allColumns.split(","));//源各个字段 mm1.put("connection", rconnList);//源连接信息 mm1.put("username", "$&#123;username&#125;");//源用户名 mm1.put("password", "$&#123;password&#125;");//源密码 mm1.put("where", "1=1");//源条件 if (StringUtils.isNoneEmpty(pkColumn)) &#123; mm1.put("splitPk", pkColumn);//源分割字段 &#125; if ("add".equals(flag)) &#123;//增量条件 if (StringUtils.isNoneEmpty(addColumn)) &#123; String[] addCol = addColumn.split(";"); String add1 = addCol[0]; String add2 = addCol[1]; mm1.put("where", String.format("%s&gt;=date_sub(now(),interval 2 day) or %s&gt;=date_sub(now(),interval 2 day)", add1, add2)); &#125; &#125; Map&lt;String, Object&gt; mmm1 = new HashMap&lt;&gt;(); mmm1.put("name", "mysqlreader");//源数据源 mmm1.put("parameter", mm1);//源参数 Map&lt;String, Object&gt; mmmm1 = new HashMap&lt;&gt;(); mmmm1.put("reader", mmm1); //********writer部分******** List&lt;Object&gt; list22 = new ArrayList&lt;&gt;(); String[] split = allColumns.split(","); //给表字段增加类型 for(int i = 0;i &lt; split.length;i++)&#123; Map&lt;String, Object&gt; m22 = new HashMap&lt;&gt;(); m22.put("name", split[i]); m22.put("type", "string"); list22.add(m22); &#125; Map&lt;String, Object&gt; m2 = new HashMap&lt;&gt;(); m2.put("jdbcUrl", "jdbc:postgresql://127.0.0.1:3433/appExtend");//目标jdbc信息 List&lt;String&gt; wtableList = new ArrayList&lt;&gt;(); String ods_table = "ods." + target_table;//全量表 List&lt;Object&gt; wconnList = new ArrayList&lt;&gt;(); wconnList.add(m2); Map&lt;String, Object&gt; mm2 = new HashMap&lt;&gt;(); //System.out.println(mm22); //System.out.println(list22); mm2.put("column", list22);//目标各个字段 //mm2.put("connection", wconnList);//目标连接信息// mm2.put("username", "tttt");//目标用户名// mm2.put("password", "tttttt");//目标密码 mm2.put("isCompress", "$&#123;isCompress&#125;"); mm2.put("defaultFS", "$&#123;hdfsPath&#125;"); mm2.put("fieldDelimiter", "$&#123;fieldDelimiter&#125;"); mm2.put("fileName", "$&#123;fileName&#125;"); mm2.put("fileType", "$&#123;fileType&#125;"); mm2.put("path", "$&#123;path&#125;"); mm2.put("writeMode", "$&#123;writeMode&#125;"); String stg_table = "stg." + target_table;//增量临时表 if ("add".equals(flag)) &#123; wtableList.add(stg_table); mm2.put("preSql", new String[]&#123;String.format("truncate table %s;", stg_table)&#125;);//执行语句之前操作 if (StringUtils.isNoneEmpty(pkColumn) &amp;&amp; StringUtils.isNoneEmpty(addColumn)) &#123;//执行语句之后操作 mm2.put("postSql", new String[]&#123;String.format("delete from %s a where exists (select 1 from %s b where a.%s=b.%s);insert into %s select * from %s;", ods_table, stg_table, pkColumn, pkColumn, ods_table, stg_table)&#125;); &#125; else &#123; mm2.put("postSql", new String[]&#123;String.format("delete from %s;insert into %s select * from %s;", ods_table, ods_table, stg_table)&#125;); &#125; &#125; else &#123; wtableList.add(ods_table); //mm2.put("preSql", new String[]&#123;String.format("truncate table %s;", ods_table)&#125;);//执行语句之前操作 &#125; m2.put("table", wtableList);//目标表名 Map&lt;String, Object&gt; mmm2 = new HashMap&lt;&gt;(); mmm2.put("name", "hdfswriter");//目标数据源 mmm2.put("parameter", mm2);//目标参数 mmmm1.put("writer", mmm2); List&lt;Object&gt; contentList = new ArrayList&lt;&gt;(); contentList.add(mmmm1); Map&lt;String, Object&gt; m3 = new HashMap&lt;&gt;(); m3.put("content", contentList); Map&lt;String, Object&gt; m4 = new HashMap&lt;&gt;(); m4.put("channel", "2"); Map&lt;String, Object&gt; mm4 = new HashMap&lt;&gt;(); mm4.put("speed", m4); m3.put("setting", mm4); Map&lt;String, Object&gt; m5 = new HashMap&lt;&gt;(); m5.put("job", m3); String str = JSON.toJSONString(m5); //JSON格式化 JSONObject object = JSONObject.parseObject(str); str = JSON.toJSONString(object, SerializerFeature.PrettyFormat, SerializerFeature.WriteMapNullValue, SerializerFeature.WriteDateUseDateFormat); return str; &#125;&#125; 此时如果大家看了代码的话可能会有一个疑惑了。这个mysqlReader.txt到底是什么东西？ 4.生成mysqlReader.txt​ 其实这个mysqlReader.txt对我们来说是个很重要的一部分，因为它里面包含了我们的mysql库名、表名以及目标hive表名和表字段等一系列重要的信息。 设定格式:schema|table|target_table|pk_column|columns|add_column ​ 当然我们也不可能手动去获取并拼接这些重要的信息，这时候就会用到一个 很重要的东西information_schema（有感兴趣的可以深究下），此处主要介绍如何获取上面的格式： 123456789select t.table_schema,t.table_name,concat('ods_xxx_',t.table_name) targettable,k.column_name,group_concat(c.column_name ORDER BY ORDINAL_POSITION) as col from information_schema.tables tjoin information_schema.columns c on c.table_schema=t.table_schema and c.table_name=t.table_nameleft join (select table_schema,table_name,column_name from information_schema.key_column_usage where constraint_name='primary') kon c.table_schema=k.table_schema and c.table_name=k.table_namegroup by c.table_nameorder by c.table_name; 这样就拿到了我们需要的一些库表名及字段，最后再将分隔符替换为|就好了。 注意： ​ 此处是有个坑的：group_concat（c.column_name） 之前是这样写的，但是他的字段会乱序，也就是不会按照你当时建表的字段顺序，但是datax脚本里面字段是必须一一对应的，否则会造成信息不匹配。所以将字段排序就变得很重要，最开始自己使用的是group_concat（c.column_name order by c.column_name）发现是按照字典序排序的，仍然满足不了我们的需求，后来查阅资料才发现order by ORDINAL_POSITION可以满足我们的需求。（有对ORDINAL_POSITION感兴趣的同学可以自行查阅！） ​ 接下来我们就基本达成了自动生成的目的！节约了时间又保证准确性！ 5.linux脚本的编写​ 当我们完成了这些个datax脚本的自动生成后，我们就要去进行执行同步，但是不同的业务数据库JDBC、帐号密码及库名又是不一样的，如果写死的话到时候更改又是一项费时费力的工程，所以我们这想到的办法就是将所有配置信息全部剥离出来，这样我们就做成了可配置的。既灵活方便，又准确无误！ 脚本分为两个： 一个是公共配置，主要是配置一些公共参数的传递。 另一个是业务独有的配置，例如JDBC连接、数据库表名和目标表名等一些参数。 公共配置： vim syn_base.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bashsource /etc/profileDATAX_HOME="/home/xxxx/datax"SHELL_PATH="/home/xxxx/dw_datax"SCRIPT_PATH=$&#123;SHELL_PATH&#125;/jobDATAX_LOG=$&#123;SHELL_PATH&#125;/logs/datax.log.`date "+%Y-%m-%d"`HDFS_PATH="hdfs://spark01:8020"START_TIME=$(date -d "-1 day" +%Y-%m-%d)END_TIME=$(date "+%Y-%m-%d")DT_TIME=$(date -d "-1 day" +%Y%m%d)#参数ISCOMPRESS="false"WRITEMODE="nonConflict"FIELDDELIMITER="|"FILETYPE="text"PATH_HIVE="/user/hive/warehouse/"#START_TIME="2019-08-22"#END_TIME="2019-08-23"#1:target_db_name #2:target_table_nameAppendPartition()&#123; ssh -p xxx spark01 "hive -e 'ALTER TABLE $1.$2 ADD IF NOT EXISTS PARTITION (day='$DT_TIME')'";&#125;TruncateHiveTable()&#123; ssh -p xxx spark01 "hive -e 'truncate table '$1'.'$2'";&#125;#1:target_db_name #2:target_table_name#3:MySQLUrl#4:MYSQL_USERNAME#5:MYSQL_PASSWORD#6:sourceTableName#7:jsonFilePathBaseDataxMysql2Hive()&#123; python $&#123;DATAX_HOME&#125;/bin/datax.py -p"-DtargetDBName=$1 -DtargetTableName=$2 -DjdbcUrl=$3 -Dusername=$4 -Dpassword=$5 -DsourceTableName=$6 -DhdfsPath=$HDFS_PATH -DstartTime=$&#123;START_TIME&#125; -DendTime=$&#123;END_TIME&#125; -DisCompress=$ISCOMPRESS -DwriteMode=$WRITEMODE -DfileType=$FILETYPE -DfieldDelimiter='$FIELDDELIMITER' -DfileName=$2 -Dpath=$&#123;PATH_HIVE&#125;$1.db/$2/day=$DT_TIME" $&#123;SCRIPT_PATH&#125;$7.json;&#125; 业务配置： vim xxx.sh 123456789101112131415161718192021222324252627#!/bin/bashsource /etc/profilesource /home/xxxx/dw_datax/shell/syn_base.shMYSQL_URL="jdbc:mysql:xxxx3306/xxxx"MYSQL_PASSWORD="xxxx"MYSQL_USERNAME="xxxx"TARGET_DB_NAME="xxxx"#1.目标库名 2.目标表名 3.数据源表名 4.文件路径名DataxMysql2HiveAppend()&#123; BaseDataxMysql2Hive $1 $2 $MYSQL_URL $MYSQL_USERNAME $MYSQL_PASSWORD $3 $4;&#125;#1.目标库名 2.目标表名 3.数据源表名 4.文件路径名DataXMysql2HiveOverride()&#123; AppendPartition $1 $2 #TruncateHiveTable $1 $2 DataxMysql2HiveAppend $1 $2 $3 $4;&#125;#增量同步DataXMysql2HiveOverride $TARGET_DB_NAME "xxxx_incr" "xxxx" "/ods/xxx/xxx_incr" &gt;&gt; /home/xxx/dw_datax/logs/ods/xxx/xxx_incr.log.`date "+%Y-%m-%d"`#全量同步DataXMysql2HiveOverride $TARGET_DB_NAME "/ods/xxx/xxx" "xxx" "/ods/xxx/xxx" &gt;&gt; /home/xxx/dw_datax/logs/ods/xxx/xxx.log.`date "+%Y-%m-%d"` 6.脚本调度​ 公司采用的是azkaban调度，自己编写job在执行job中写入自己的执行命令： 1sh /home/xxx/dw_datax/shell/xxx.sh ​ 最后定时调度即可。 7.遇到的问题问题梳理： 12345678910111213141516171819202122231.Datax同步过程String can&apos;t be cast List 解决：table表得放在list中2.分隔符只能是单字符（只能是|而不能是||）3.datax不支持parquet格式（ODS层采用Text，DWD层采用parquet）4.增量同步时where条件是substr截取时间快还是用大于号小于号 快5.增量同步的分区问题（用月分区还是用日分区）：最终选择用日分区：原因是月分区会每次覆盖之前的无法保存数据的状态6.在DWD层做全表的保存，而不是ODS层。7.水平分区和嵌套分区的区别（yyyymmdd和yyyy/mm/dd）的区别8.sed的使用9.nodepad++ 行尾替换$ 删除当前行Ctrl+l10.python传参11.mysql中information_schema的用处及ORDINAL_POSITION的排序select t.table_schema,t.table_name,concat(&apos;ods_exp_&apos;,t.table_name) targettable,k.column_name,group_concat(c.column_name ORDER BY ORDINAL_POSITION) as col from information_schema.tables tjoin information_schema.columns c on c.table_schema=t.table_schema and c.table_name=t.table_nameleft join (select table_schema,table_name,column_name from information_schema.key_column_usage where constraint_name=&apos;primary&apos;) kon c.table_schema=k.table_schema and c.table_name=k.table_namegroup by c.table_nameorder by c.table_name;12.Easy Scheduler13.datax 脚本的梳理（增量全量）14.azkaban的编写 最后送给自己一段话： 只有不断坚持，才能不断成长！]]></content>
  </entry>
  <entry>
    <title><![CDATA[windows10系统安装]]></title>
    <url>%2F2019%2F09%2F09%2Fwindows10%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[windows系统安装背景： ​ 由于自己不小心格式化了C盘，因此必须重装系统！自己电脑是拯救者R720.特来梳理一下步骤增加印象。 ​ 注意：注意自己如何进入bois（我的是F2），以及如何设置U盘启动（我的是F12），以及UEFI和Legacy的区别等。 1.准备工作： 一个U盘（空不空无所谓，到后面都要被格式化） 由于C盘已经格式化，所以无法进行开机。因此此时需要一个U盘来做启动盘！ 另外一台有网电脑 主要是用来下载一个老毛桃软件，同时为U盘做做启动盘。也为稍后下载驱动做准备！ win10原版镜像（我的是专业版）下载 下载地址为：https://msdn.itellyou.cn/（推荐这个，网上下载地址太多） 2.做启动盘：​ 具体步骤如下链接： ​ http://laomaotao.tzwyo.cn/upqdzz.html ​ 1. 到老毛桃官网中下载老毛桃v9.3安装包到系统桌面上，如图是已经下载好的老毛桃安装包： ​ 2. 鼠标左键双击运行安装包，接着在“安装位置”处选择程序存放路径（建议大家默认设置安装到系统盘中），然后点击“开始安装”即可，如下图所示： ​ 3. 随后进行程序安装，我们只需耐心等待自动安装操作完成即可，如下图所示： ​ 4. 安装完成后，点击“立即体验”按钮即可运行u盘启动盘制作程序，如下图所示： ​ 5. 打开老毛桃u盘启动盘制作工具后，将u盘插入电脑usb接口，程序会自动扫描，我们只需在下拉列表中选择用于制作的u盘，然后点击“一键制作”按钮即可，如下图所示： ​ 6. 此时会弹出一个警告框，提示“警告：本操作将会删除 I：盘上的所有数据，且不可恢复”。在确认已经将重要数据做好备份的情况下，我们点击“确定”，如下图所示： ​ 7. 接下来程序开始制作u盘启动盘，整个过程可能需要几分钟，大家在此期间切勿进行其他操作，如下图所示： ​ 8. U盘启动盘制作完成后，会弹出一个窗口，提示制作启动U盘成功。要用“模拟启动”测试U盘的启动情况吗？我们点击“是”，如下图所示： ​ 9. 启动“电脑模拟器”后我们就可以看到u盘启动盘在模拟环境下的正常启动界面了，按下键盘上的“Ctrl+Alt”组合键释放鼠标,最后可以点击右上角的关闭图标退出模拟启动界面，如下图所示： 3.安装系统：​ 具体步骤如下链接： ​ http://laomaotao.tzwyo.cn/upzybwin10.html ​ 1. 将制作好的老毛桃装机版启动u盘插入电脑usb接口（如果是台式机，建议插在主机箱的后置接口），然后开启电脑，等到屏幕上出现开机画面后按快捷键进入到老毛桃主菜单页面，接着将光标移至“【02】老毛桃WIN8 PE标准版（新机器）”，按回车键确认，如下图所示： 2. 进入pe系统后，鼠标左键双击打开桌面上的老毛桃PE装机工具。打开工具主窗口后，点击映像文件路径后面的“浏览”按钮，如下图所示： 3. 此时会弹出一个查找范围窗口，我们只需打开启动u盘，选中win10 iso系统镜像文件，点击“打开”按钮，如下图所示 4. 随后我们根据需要在映像文件路径下拉框中选择win10系统其中一个版本（这里，以win10系统专业版为例），接着在磁盘分区列表中选择C盘作为系统盘，然后点击“确定”按钮即可，如下图所示： 5. 此时会弹出一个提示框，询问是否需执行还原操作，在这里建议默认设置，只需点击“确定”即可，如下图所示： 6. 完成上述操作后，程序开始释放系统镜像文件。释放完成后，电脑会自动重启，继续余下的安装操作，我们只需耐心等待即可。如下图所示： 4.下载驱动：​ 启动好电脑之后我们会发现电脑并没有办法上网（原因就是电脑没有网卡和无线网卡），此时我们需要下载驱动人生将我们电脑上的一些配置都导出来，然后用有网的电脑下载好驱动，然后再拷贝过来进行离线安装！ 电脑连上网之后我们就可以进行一系列的操作了！]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo + github搭建（基础）]]></title>
    <url>%2F2019%2F04%2F28%2Fhexo-github%E6%90%AD%E5%BB%BA%EF%BC%88%E5%9F%BA%E7%A1%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[温馨提示：电脑系统为window 10专业版，64位 具体步骤1、安装Node.js和配置好Node.js环境node.js下载地址 2、安装Git和配置好Git环境（成功的标志为在电脑上任何位置鼠标右键能够出现Git GUI Here和Git Bash Here两个选择）注意：如需要在cmd中调用Git命令，则需配置电脑环境变量Path(输入 git - -version 验证) Git地址 3、Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io，不然接下来会有很多麻烦。并且需要勾选Initialize this repository with a README在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会发现该项目已经被部署到网络上，能够通过外网来访问它。 4、安装Hexo，在自己认为合适的地方创个文件夹，我是在E盘建了一个blog文件夹。然后通过命令行进入到该文件夹里面 输入npm install hexo -g，开始安装Hexo 输入hexo -v，检查hexo是否安装成功 输入hexo init，初始化该文件夹（需要等待一会,如果卡主下面有解决办法！！！） 输入npm install，安装所需要的组件 输入hexo g，首次体验Hexo 输入hexo s，开启服务器，访问该网址，正式体验Hexo 问题：假如页面一直无法跳转，那么可能端口被占用了。此时我们ctrl+c停止服务器，接着输入“hexo server -p 端口号”来改变端口号 注意：用 npm 安装话经常出现卡住而导致无法正常安装，解决办法就是修改 npm 的安装源，这里选择淘宝 NPM 镜像，这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步。。 npm config set registry https://registry.npm.taobao.org 5、将Hexo与Github page联系起来，设置Git的user name和email（如果是第一次的话）在其文件夹里面鼠标右键，点击Git Base Here 12git config --global user.name &quot;yourname&quot;git config --global user.mail &quot;yourname@mail.com&quot; 这里”yourname”可以替换成自己的用户名，邮箱可以替换成自己的邮箱 输入cd ~/.ssh，检查是否由.ssh的文件夹 输入ssh-keygen -t rsa -C “yourname@mail.com“，连续三个回车，生成密钥，最后得到了两个文件：id_rsa和id_rsa.pub（默认存储路径是：C:\Users\Administrator.ssh）。 输入eval “$(ssh-agent -s)”，添加密钥到ssh-agent 再输入ssh-add ~/.ssh/id_rsa，添加生成的SSH key到ssh-agent登录Github，点击头像下的settings，添加ssh新建一个new ssh key，将id_rsa.pub文件里的内容复制上去输入ssh -T git@github.com，测试添加ssh是否成功。如果看到Hi后面是你的用户名，就说明成功了 问题：假如ssh-key配置失败，那么只要以下步骤就能完全解决首先，清除所有的key-pair12ssh-add -Drm -r ~/.ssh 删除你在github中的public-key 重新生成ssh密钥对ssh-keygen -t rsa -C &quot;yourname@mail.com&quot; 接下来正常操作在github上添加公钥public-key: - 首先在你的终端运行 xclip -sel c ~/.ssh/id_rsa.pub将公钥内容复制到剪切板 - 在github上添加公钥时，直接复制即可 - 保存 测试：在终端 ssh -T git@github.com 6、配置Deployment，在其文件夹中，找到_config.yml文件，修改repo值（在末尾）1234deploy: type: git repository: git@github.com:xxxxxx/xxxxxxx.github.io.git branch: master repo值是你在github项目里的ssh（右下角) 7、新建一篇博客，在cmd执行命令：hexo new post “博客名”” 这时候在文件夹_posts目录下将会看到已经创建的文件 在生成以及部署文章之前，需要安装一个扩展： npm install hexo-deployer-git --save 使用编辑器编好文章，那么就可以使用命令： hexo d -g 生成以及部署了 部署成功后访问你的地址：http://用户名.github.io。那么将看到生成的文章 好了，到此为止，最基本的hexo+github搭建博客完结。]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
